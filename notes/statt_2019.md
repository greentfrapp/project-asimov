# AI voice assistants reinforce harmful gender stereotypes, new UN report says

[Link to article](https://www.theverge.com/2019/5/21/18634322/amazon-alexa-apple-siri-female-voice-assistants-harmful-gender-stereotypes-new-study)

[Link to UN report](https://unesdoc.unesco.org/ark:/48223/pf0000367416.page=1) *This definitely deserves a detailed read.*

[Link to Medium PCMag article *The Real Reason Voice Assistants Are Female*](https://medium.com/pcmag-access/the-real-reason-voice-assistants-are-female-and-why-it-matters-e99c67b93bde) *This article contains some interesting psychological analyses on preferences for gendered voices.*

This is highlights an interesting point that is related to nudging (see Sunstein and Thaler's *Nudge*) and the politics of artifacts (see Winner's *Do Artifacts have Politics?*). Gendered voice assistants appear to be a feedback loop - companies choose female voices because they appeal more to consumers, which reinforces the stereotype of the subservient female, in turn encouraging companies to continue using female voices.

On my first glance, this appears to be more a general technology design problem rather than one that is unique to AI. Will need to look at the UN report in further detail. 

Both this article and the UN report cites the predominance of white male AI researchers as a possible reason for this problem. which is a rehashing of AI (and technology's) diversity crisis. Not sure if this is also unique to AI but definitely worth considering - is this an important source of other problems (e.g. algorithmic bias)?

[Link to other Verge article on diversity crisis](https://www.theverge.com/2019/4/16/18410501/artificial-intelligence-ai-diversity-report-facial-recognition) *This article links to several other interesting ones, including an article on the famous AI gaydar paper.*

[Link to AI Now Institute's report on diversity crisis](https://ainowinstitute.org/discriminatingsystems.pdf)
