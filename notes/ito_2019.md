# Supposedly 'Fair' Algorithms Can Perpetuate Discrimination

[Link to article](https://www.wired.com/story/ideas-joi-ito-insurance-algorithms/)

Joi Ito begins with the issue of *redlining*, which is actually really interesting. Redlining began with financial institutions drawing red lines around districts that they do not lend to or insure for. Since these districts were mainly black and poor, the practice reeks of discrimination. However, the banning of the practice was countered by the financial institutions which cited statistical reasons why lending or insuring residents from these districts did not make good business sense. Subsequently, in the 2008 financial crisis, some stakeholders blamed the subprime mortgage crisis partly on the government, who had "encouraged" the banks to lend to individuals who were redlined. Is there some truth in the statistical analysis? And even if there is, should we "sacrifice fairness" for it?

Ito raises many interesting problems with the idea of fairness in this article - conflating fairness with accuracy, feedback loops and self-fulfilling prophecies. One important point that was implied but not made explicit - many times, the practice does not serve the ultimate goal. For instance, from a societal point of view, the practice of putting criminals in jail might serve to ultimately make the society a better place via deterrence and reformation. However, jails are often ineffective as deterrence and poor at reformation. More importantly, the algorithms behind the predictions typically optimize for prediction accuracy rather than societal safety. How can we better design recidivism prediction algorithms to optimize for societal safety?
