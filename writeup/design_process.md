## Design Process

This section outlines the general design process of the online primer at [https:machinesgonewrong.com](https://machinesgonewrong.com). When designing the primer, we aimed to take advantage of the interactivity afforded by the online medium to improve the learning experience. We also utilized styles and memes that were familiar to the target audience, coupled with a casual tone, to convey the learning points in an accessible manner.

### The Structure

The online primer has two main objectives - to help the reader understand the importance of AI ethics and to introduce fundamental concepts in algorithmic bias. The structure of the guide was crafted to reflect these objectives. In the first section *Getting Started*, we look at what we mean when we use the terms "AI ethics" and "AI systems". We then explain what is different about AI, elaborating on the three differences mentioned previously (illusion of fairness, speed and scale, and accessibility). The next section is dedicated entirely to algorithmic bias. We first look at non-mathematical and mathematical definitions of fairness in *Understanding Fairness*. Then we examine algorithmic bias in detail in the subsections *Understanding Bias I* and *Understanding Bias II*. The first subsection uses a framework proposed by Kate Crawford in her NIPS2017 keynote presentation <dt-cite cite="crawford2017trouble"></dt-cite>, which divides bias into harms of allocation and harms of representation. In *Understanding Bias II*, we look more closely at the design process of an AI system and identify common sources of bias at each step of the process. This section takes reference from Suresh and Guttag, Barocas and Selbst, and Selbst et al. <dt-cite cite="suresh2019framework,barocas2016big,selbst2019fairness"></dt-cite>, all of which look at sources of bias from different perspectives. The primer then presents a checklist of questions that summarizes the lessons from the previous sections. Finally, we consolidate a short list of resources for readers who are interested in looking at algorithmic bias in more detail. These resources include software tools, datasets and related academic publications.

### The Online Medium

One focus of the primer was to take advantage of interactive capabilities enabled by the medium we are using - i.e. the online website. In particular, Bret Victor's popular essay *Explorable Explanations* considered several ways that an educational resource could make use of online elements to enhance the learning experience <dt-cite cite="victor2011explorable"></dt-cite>, including reactive documents, explorable examples and contextual information. The online primer uses all these elements to varying degrees. Various examples were also referenced from the explorabl.es repository of explorable explanations and the Distill journal, which is an online-only computer science academic journal that focuses on relatable publications with explorables.

<!-- Add diagrams here -->

#### A Fair Fat Pet Predictor

The *Understanding Fairness - A Fair Fat Pet Predictor* section uses an explorable explanation set in the context of a fat pet predictor, in order to explain several fairness definitions. Most of these definitions are described in detail in academic reviews such as Verma and Rubin's *Fairness Definitions Explained* <dt-cite cite="verma2018fairness"></dt-cite>. However, the format of such publications can be non-intuitive and tedious to read, especially when there are many definitions differing in subtle but significant ways. In the online primer, we use an interactive pie chart that shows the breakdown of the fat pet predictor's accuracies. Adjusting this pie chart changes the accuracy of the predictor on different levels and results in the predictor fulfilling different fairness definitions. This interactive exploration is meant to help readers better understand the fairness definitions. Moreover, readers playing with the example will find it impossible to obtain a configuration that fulfills all the fairness definitions. This ties in to the next subsection, which talks about the impossibility theorem of fairness and how some fairness definitions are fundamentally incompatible.

#### Google Image Search iframes

In the *Understanding Bias I - Harms of Representation*section, there are several Google Image Search iframes that allow readers to do a live search of the relevant search terms. This borrow from Bret Victor's contextual information element, which is meant to allow readers to verify a source's claims through easy access to contextual information. In this case, the primer discusses the stereotypes embodied by Google Image search results for certain search terms, such as photos of white males for the search term "CEO". The iframes allow readers to easily verify the described phenomenon and observe how the search results are now slightly more diverse. Readers can also key in their own search terms to explore other possible terms that might surface similar stereotypes.

#### Tidbits

Finally, the online primer also features expandable content or what we call "tidbits". Often, it can be difficult to strike a balance between core information and excessive details. Including too much detail can make a text seem too verbose and discourage readers from finishing. On the other hand, sticking to only the core information might be too shallow for interested readers. *Tidbits* are presented to the reader in the form of red circles with a plus sign that can be clicked to reveal additional material. This additional material can take the form of a more detailed elaboration or more illustrative examples. In the *Getting Started - The Most Important Question* section, tidbits are used to elaborate on some of the neglected ripples when implementing AI systems. In the *Understanding Fairness - Disparate Treatment and Disparate Impact* section, tidbits are used to provide examples on sources of disparate impact. In addition, in the *Understanding Fairness - A Fair Fat Pet Predictor* section, this mechanism is used to hide an optional subsection on true/false positives/negatives that might already be very familiar to readers who are AI practitioners.

Finally, the tidbit can also be used to unveil a new perspective on existing material. For example, clicking on the tidbit in the *Getting Started - Ethics of Artificial Intelligence* section prepends the words "human-designed" to all instances of "AI" on the page, as a way of reminding readers that current AI systems are designed by human practitioners.

### Relatable Content

Since the online primer is targeted at all AI practitioners - researchers, engineers and hobbyists, we chose to adopt a primarily casual and informal tone to convey our subject matter. This was designed into the primer in two main ways - the language used and the accompanying images.

#### Language Used

We focused on ensuring that the language used is relatable and easy to understand. A recent New York Times article by Kevin Litman-Navarro vetted 150 privacy policies using a tool known as the Lexile test <dt-cite cite="litman2019we"></dt-cite>, which scored the readability of texts based on sentence length and vocabulary used. As described by Litman-Navarro, "[the] vast majority of these privacy policies exceed the college reading level". A text of college reading level would score about 1300 to 1400 on the Lexile test, with higher scores referring to more complex texts. In the case of the online primer, we aimed for a Lexile test score of 1100 to 1200, which is at the higher end of the high school spectrum. *Due to restrictions on the free Lexile test account that we are using, we are unable to publicly disclose the scores that we have achieved.*

#### Accompanying Images

To set the informal tone of the primer, we also made use of image references to well-known memes and popular media. Examples include a GIF from the movie *The Incredibles* and an adaptation of the bongo cat meme. In addition, the comics in the primer also borrowed heavily from Randall Munroe's xkcd comics, which had a technical-science-inclined audience, such as programmers, computer scientists and physicists. This was meant to help the primer relate to its target audience of AI practitioners. As a bonus, the xkcd comics had a stickman style that was relatively easier to draw. The comics in the online primer also have title texts that would show upon hovering, similar to Munroe's xkcd comics.

### Visual Accessibility

One important consideration was designing for accessibility, especially since a big part of AI ethics concerns accessibility of the technology to a diverse population. Specifically, we paid attention to designing for visual accessibility in terms of the colors used and the color contrast between texts and their background.

For the pie charts in the *Understanding Fairness - A Fair Fat Pet Predictor* section, the chosen hues of red and blue are meant to be sufficiently distinct for readers who have either of the two main types of color deficiencies - dichromatism and anomalous trichromatis <dt-cite cite="jefferson2006accommodating"></dt-cite>. Tooltips are also present to help guide readers who have the rarer third type of color deficiency known as monochromatism or complete colour blindness.

For the color contrasts of texts, we observed the standards established by the Web Content Accessibility Guidelines (WCAG), which stipulate contrast ratio requirements of 4.5 to 1 for Level AA and 7 to 1 Level AAA for normal-sized texts <dt-cite cite="caldwell2008web"></dt-cite>. In the design of the guide, we ensured that all texts minimally observed Level AA in terms of color contrast, with the vast majority observing level AAA.

Finally, the guide also allows readers to switch from regular mode to dark mode. This provides a more comfortable alternative for readers who might experience visual discomfort with the usual black-on-white text with its bright background.

### Feedback

After an initial draft of the online guide was completed, a link to the guide was shared amongst personal contacts who were also AI practitioners. The draft was also posted on the r/artificial, r/AIEthics and r/MachineLearning subreddits. Many of the responses took the form of constructive feedback that provided new resources or important problems with the online primer. These included additional resources such as publications on the LessWrong blog and Yarden Katz's *Manufacturing an Artificial Intelligence Revolution*. One response raised to our attention that training separate models for separate groups (a suggestion in the guide referenced from Suresh and Guttag <dt-cite cite="suresh2019framework"></dt-cite>) was an illegal practice known as subgroup norming, under USA's labor laws.

A particularly detailed response also highlighted some important issues with the primer, such as the lack of diversity in the examples and resources, which were primarily Western and American-centric.

Of note, on the r/MachineLearning subreddit, there was a significant discussion about the concerns over adapting the style of Randall Munroe's xkcd comics, which eventually led us to modify the comics and distance the association. However, there were also a few interesting takeaways. The intense discussion with several voices and opinions suggests that the xkcd comics are indeed popular in the community and caught the attention of the readers. More interestingly, the discussion also reminded us that AI ethics, just like general ethics, can be a sensitive topic. Specifically, authors writing on AI ethics might be subject to additional scrutiny due to the nature of the subject matter.
