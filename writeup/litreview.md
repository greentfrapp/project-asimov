## Literature Review

<!--
- Structural Biases
- Systematic Oversight
 -->

### Structural Biases in Computer Science

The prevalance of bias in algorithmic systems can partly be traced back to the structural biases within the computer science field. Such biases comprise of gender and racial imbalances, as well as differential treatment along gender and racial lines. These biases create environments and workspaces that are not conducive for the design of inclusive systems and solutions.

Structural biases in computer science have been well-documented along gender and racial lines <dt-cite cite="catsambis1994path,tang1997glass,ong2011inside,sax2017anatomy,sosnowski2002women"></dt-cite>. In a statistical analysis of North American institutions, women make up just 15% of tenure-tracked computer science faculty <dt-cite cite="clauset2015systematic"></dt-cite>. Way et al. suggested that this gender bias might not be a result of explicit discrimination based on gender <dt-cite cite="way2016gender"></dt-cite>. Instead, Way et al. explained that "the effects of gender are indirectly incorporated into hiring decisions through gender's covariates".

In the US in 2015, "girls represented only 22 percent and underrepresented minorities only 13 percent of the approximately 50,000 students who took the Advanced Placement Computer Science (AP-CS) exam nationally" <dt-cite cite="munoz2015as"></dt-cite>. In response, the Obama Administration initiated the Computer Science for All initiative seeking for "more inclusive and accessible [Computer Science (CS)] curriculum" <dt-cite cite="smith2016computer"></dt-cite>. In a 2016 report, Singapore's Info-communications Media Development Authority (IMDA) documented the gender imbalance in the infocommunications (infocomm) industry, with only 30% of infocomm professionals being female <dt-cite cite="imda2017annual"></dt-cite>. A 2014 report by code-sharing website CodeForge showed similar gender imbalance amongst Chinese software professionals, with 20% of Chinese programmers being female <dt-cite cite="codeforge2014"></dt-cite>. Code-sharing website GitHub is popular in the international coding community and has over 5 million developers across 10 million project repositories <dt-cite cite="gousios2014lean"></dt-cite>.

In a work by Terrell et al. analyzing bias in open-source programming, gender bias is manifested in two main ways on GitHub <dt-cite cite="terrell2017gender"></dt-cite>. First, the authors only managed to identify approximately 21,000 female users, compared to approximately 310,000 male users. While the authors' target group was non-exhaustive, this reflects a gender imbalance amongst GitHub's users. Second, the authors found that contributions from female users tend to be accepted more frequently than male users when the female's gender is non-identifiable. In contrast, when a female user's gender is identifiable, they are rejected more often.

All of these form part of a larger trend of a systematic bias against certain groups in the computer science field. In turn, such biases have the potential to be embedded in the solutions and products designed by computer science professionals. Without a diverse environment, designers and engineers are more likely to be subject to echo chambers and neglect the needs of the underrepresented. Such environments are also likely to cultivate behavior that harms members of the underrepresented groups. Kabat-Farr and Cortina showed how male-dominated workspaces often record higher incidence of female harrassment and discrimination <dt-cite cite="kabat2014sex"></dt-cite>.

### Structural Biases in the Industry

Beyond the racial and gender biases that plague the computer science field, other structural biases also exist in the larger technology industry. When commercializing AI solutions, business owners have to weight ethical considerations against other moativations. In particular, monetary incentives play a naturally outsized role in the technology industry. More often than not, these monetary incentives can be at odds with the ideals of fairness and altruism.

A Bloomberg article in 2016 reported on apparent racial disparities in the availability of Amazon's Prime Free Same-day Delivery service <dt-cite cite="ingold2016amazon"></dt-cite>. The article writes: "Amazon says its plan is to focus its same-day service on ZIP codes where there's a high concentration of Prime members, and then expand the offering to fill in the gaps over time." This might make sense from a business point of view. However, this neglects potential correlations between density of Prime members and density of different ethnic groups in the district. This decision effectively resulted in the service being denied from districts with higher proportion of black residents. By focusing exclusively on monetary goals, ethical ideals such as fairness might be sacrificed, whether intentionally or not.

A classic example with a modern twist is digital redlining. Traditional redlining refers to "when one or more lending institutions decline  to loan in a specific area, or demand terms that are so stringent as to be prohibitive" <dt-cite cite="harris2003suburban"></dt-cite>. The term originates from the red lines used on maps to demarcate districts where loans should be denied. Where classic redlining painted broad strokes based on geographical districts, modern digital redlining enables service providers to target specific demographics, based on gender, ethnicity, age, political views and general consumer habits and preferences. This has been enabled by the proliferation of social media platforms and tracking of Internet browsing history by service providers.

A 2016 expos√© from ProPublica revealed that advertisers on Facebook had the option to exclude targeted users by race <dt-cite cite="angwin2016facebook"></dt-cite>. In a Washington Post article, Amazon was found to have priced their DVDs differently for different buyers <dt-cite cite="streitfeld2000we"></dt-cite>. A review by Akiva A. Miller covered a wide range of discriminative pricing strategies, including the use of consumer data from data brokers, purchase history, geographical location and browsing information <dt-cite cite="miller2014we"></dt-cite>. Furthermore, Bar-Gill contends that such digital redlining strategies can be further distorted by misperceptions based on awry data or interpretations <dt-cite cite="bar2018algorithmic"></dt-cite>.

In redlining practices, it is often unclear whether the discrimination is an unintended by-product of monetary pursuits or intentionally hidden behind layers of rationalization, termed "masking" by Barocas and Selbst <dt-cite cite="barocas2016big"></dt-cite>. But intentional or not, the injustice is real and often harms minority groups who are already more vulnerable. While such discriminative selling strategies have been practiced for decades <dt-cite cite="hernandez2009redlining,harris2003suburban"></dt-cite>, modern algorithms enable business owners to employ them with greater precision, on a far larger scale and hidden behind more complicated and abstract equations. 

### Ethics in Engineering Pedagogy

<!-- Refer to Sun's previous link on was it Harvard or some other institution. Check for more diverse sources. -->

Finally, e