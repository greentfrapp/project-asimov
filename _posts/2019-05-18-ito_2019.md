---
layout: post
title: "Supposedly 'Fair' Algorithms Can Perpetuate Discrimination"
ref: ito_2019
date: 2019-05-20 00:00:02
tags: fairness law-enforcement finance
notes: True
---

# Supposedly 'Fair' Algorithms Can Perpetuate Discrimination

[Link to article](https://www.wired.com/story/ideas-joi-ito-insurance-algorithms/)

Joi Ito begins with the issue of *redlining*, which is actually really interesting. Redlining began with financial institutions drawing red lines around districts that they do not lend to or insure for. Since these districts were mainly black and poor, the practice reeks of discrimination. However, the banning of the practice was countered by the financial institutions which cited statistical reasons why lending or insuring residents from these districts did not make good business sense. Subsequently, in the 2008 financial crisis, some stakeholders blamed the subprime mortgage crisis partly on the government, who had "encouraged" the banks to lend to individuals who were redlined. Is there some truth in the statistical analysis? And even if there is, should we "sacrifice fairness" for it?

Ito raises many interesting problems with the idea of fairness in this article - conflating fairness with accuracy, feedback loops and self-fulfilling prophecies. One important point that was implied but not made explicit - many times, the practice does not serve the ultimate goal. For instance, from a societal point of view, the practice of putting criminals in jail might serve to ultimately make the society a better place via deterrence and reformation. However, jails are often ineffective as deterrence and poor at reformation. More importantly, the algorithms behind the predictions typically optimize for prediction accuracy rather than societal safety. How can we better design recidivism prediction algorithms to optimize for societal safety?

> In an attempt to improve recovery from the riots and to address the role redlining may have played in them, President Lyndon Johnson created the President's National Advisory Panel on Insurance in Riot-Affected Areas in 1968. The report from the panel showed that once a minority community had been redlined, the red line established a feedback cycle that continued to drive inequity and deprive poor neighborhoods of financing and insurance coverage—redlining had contributed to creating poor economic conditions, which already affected these areas in the first place.

Similar feedback loops can be observed when recidivism prediction algorithms are used to sentence criminals.

> Using gender to determine risk classification, they claimed, was fair; the statistics they used showed a strong correlation between gender and the outcomes they insured against.

> But fairness and accuracy are not necessarily the same thing. For example, when [Julia Angwin pointed](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) out in her ProPublica report that risk scores used by the criminal justice system were biased against people of color, the company that sold the algorithmic risk score system argued that its scores were fair because they were accurate. [...] But this correlation contributes to discrimination, because using arrests as a proxy for recommitting a crime means the algorithm is codifying biases in arrests, such as a police officer bias to arrest more people of color or to patrol more heavily in poor neighborhoods. This risk of recidivism is used to set bail and determine sentencing and parole, and it informs predictive policing systems that direct police to neighborhoods likely to have more crime.

> There are several obvious problems with this. If you believe the risk scores are accurate in predicting the future outcomes of a certain group of people, then it means it's "fair" that a person is more likely to spend more time in jail simply because they are black. This is actuarially "fair" but clearly not "fair" from a social, moral, or anti-discrimination perspective.

> The other problem is that there are fewer arrests in rich neighborhoods, not because people there aren't smoking as much pot as in poor neighborhoods but because there is less policing. Obviously, one is more likely to be rearrested if one lives in an overpoliced neighborhood, and that creates a feedback loop—more arrests mean higher recidivism rates. In very much the same way that redlining in minority neighborhoods created a self-fulfilling prophecy of uninsurable communities, overpolicing and predictive policing may be "fair" and "accurate" in the short term, but the long-term effects on communities have been shown to be negative, creating self-fulfilling prophecies of poor, crime-ridden neighborhoods.

> So while redlining for insurance is not legal, when [Amazon decides to provide Amazon Prime free same-day shipping to its “best” customers](https://www.bloomberg.com/graphics/2016-amazon-same-day/), it’s effectively redlining — reinforcing the unfairness of the past in new and increasingly algorithmic ways.
