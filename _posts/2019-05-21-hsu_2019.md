---
layout: post
title: "Machine Learning Predicts Kids at Risk of Not Getting Vaccinated"
ref: hsu_2019
date: 2019-05-21 00:00:01
tags: healthcare
notes: True
---

# Machine Learning Predicts Kids at Risk of Not Getting Vaccinated

[Link to article](https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/machine-learning-predicts-kids-at-risk-of-not-getting-vaccinated)

This articles presents a nice example of how algorithms are used in an important social policy (advising families skeptical of vaccines), with rather comprehensive considerations for social issues.

> After comparing the results from four machine learning models, researchers decided upon a LASSO logistic regression model that identified vaccine-hesitant families with 72-percent precision. **The model pruned the large number of possible data features affecting vaccination rates down to just 25 of the most important features — something that improved the chance of the model's predictive power holding up for other groups of children beyond those in the training datasets.** (Some features that raised child risk scores included having children who sat, walked, and spoke at a later age than their peers.)

This shows a consideration for the transferability of the model to other populations.

> Just as importantly, the team chose the LASSO model because it presented the results for child risk scores in a way that humans could understand. Interpretability is never a guarantee with many machine learning models, but in this case it allowed both data scientists and health officials to understand and trust the LASSO model's reasons for singling out certain families as being at higher risk of hesitating to vaccinate.

The LASSO model might not be the most accurate, but the interpretability of the model is of top priority in this problem. Another important note that is implied here is the communication between data scientists and health officials.

> The project also created an "Early Warning and Monitoring System" Web dashboard that presents vaccination rates and child risk scores to public health officials and physicians at national, county, and local health clinic levels. The next project being considered will likely involve a randomized controlled trial to see whether the child risk scores help officials and physicians to intervene effectively with vaccine-hesitant families and improve vaccination rates.

Beyond just designing and training the algorithm, there is follow-up action to evaluate the effectivesness of the "child risk scores".

> Some important issues have to be resolved before this type of predictive population analysis can be widely deployed. Any project that applies machine learning or related artificial intelligence techniques to analyzing personal health data has to take privacy and security concerns into consideration. In this case, Oreskovic's team and Croatian public health officials took special precautions to ensure that the electronic health records of children were always made anonymous. The researchers accessed the records through the Croatian Institute for Public Health's online server and never even downloaded any anonymized data.

This demonstrates a healthy regard for privacy, although it is important to note that trivial anonymity is sometimes insufficient to guarantee privacy.

> Another issue for data scientists to keep in mind, says Oreskovic, is whether machine learning and AI might identify data features that contribute to biased policymaker actions regarding certain communities. He cautioned against the idea of deploying models that highlight features such as ethnicity or religion—factors that were excluded from the data in the Croatian study.

> If a computer model did hypothetically flag certain religious affiliations, it raises the risk of officials acting upon the information in a way that would stigmatize entire religious groups. "The question is: Does the extra attention hurt the community, either through prejudice or too much policy intervention," Oreskovic says.

This is an interesting problem because in the case of the anti-vac movement, we know contextually that religious and political affiliations might correlate with being anti-vac. However, here Orešković appear to have chosen to downplay the effects of these features in light of the risk of stigmatization, possibly increasing the inaccuracy of the model. There is an obvious trade-off. On another note, if a policymaker intends to completely neutralize the effects of race, religion, ethnicity or any sensitive demographic information, more needs to be done beyond just excluding these features from the algorithm. One also has to look at possible correlations between sensitive and non-sensitive features, to prevent sensitive features from creeping into the data. See this article for a practical example: [Amazon Doesn't Consider the Race of Its Customers. Should It?](https://www.bloomberg.com/graphics/2016-amazon-same-day/)
