---
layout: post
title: "Addressing the Biases Plaguing Algorithms"
ref: li_2019
date: 2019-05-24 00:00:01
tags: algorithmic-bias
notes: True
---

# Addressing the Biases Plaguing Algorithms

[Link to article](https://hbr.org/2019/05/addressing-the-biases-plaguing-algorithms)

This article does not really touch on anything new but I find it a nice primer for a newcomer to the current ethical concerns dominating debates over AI applications. The article is nicely structured with examples first, before delving into "What Can We Learn From AI Failures?"

The examples include:

- Microsoft Tay ([link 1](https://www.theverge.com/2016/3/23/11290200/tay-ai-chatbot-released-microsoft); [link 2](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist))
- Facebook's Death Predictions ([link 1](https://www.npr.org/2019/04/09/711399357/facebook-promises-to-stop-asking-you-to-wish-happy-birthday-to-your-friend-who-d); [link 2](https://techcrunch.com/2016/11/11/facebook-suddenly-thinks-a-bunch-of-people-are-dead-dont-panic/))
- Google Translate being sexist ([link](https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples/))
- Amazon's Hiring AI ([link](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G))

> It's essential to keep in mind that AI is not fundamentally biased. As we've seen, the bias in these algorithms are the result of biased training data built by humans. It's not the fundamental technology that's racist or sexist, but the data on which we train the algorithms. The solution can't simply be to collect unbiased data, sadly — almost all human data is fundamentally biased in some way.

> Companies need to remain vigilant to keep bias out of their AI systems. They need to incorporate anti-bias training alongside their AI and ML training, spot potential for bias in what they’re doing, and actively correct for it. And along with our usual Q&A processes for software, AI needs to undergo an additional layer of social Q&A so that problems can be caught before they reach the consumer and result in a massive backlash. Additionally, the data scientists and AI engineers training the models need to take courses on the risks of AI.

> Most importantly, business leaders need specialized AI training to understand both the possibilities and the risks. This is not technical — executives don’t need to be hands-on practitioners — but they do need to understand data science and AI enough to manage AI products and services. Business leaders need to understand the potential for AI to transform business for the better, as well as its potential shortcomings — and dangers.

> Understanding these dangers is the responsibility of not just those leading AI initiatives, but all executives. A PR leader who understands social media dynamics and the vicious troll culture could have averted the dangers of a self-learning AI Twitter bot. An executive steeped in HR and employment discrimination law can help flag potential dangers of resume screening bots. And a manager with operating experience across multiple countries might be able to spot the sensitivity around translating genderless pronouns.

> The risks of AI can come from any aspect of business, and no single manager has the context to spot everything. Rather, in a world in which AI is permeating everything, companies need to train all their business leaders on AI’s potential and risks, so that every line of business can spot opportunities and flag concerns. The institutional know-how to spot the dangers of AI is already in your company — you just need to unleash it.
