---
layout: post
title: "Thanks to Facebook, Your Cellphone Company Is Watching You More Closely Than Ever"
ref: biddle_2019
date: 2019-05-24 00:00:01
tags: privacy fairness
notes: True
---

# Thanks to Facebook, Your Cellphone Company Is Watching You More Closely Than Ever

[Link to article](https://theintercept.com/2019/05/20/facebook-data-phone-carriers-ads-credit-score/)

> Among the mega-corporations that surveil you, your cellphone carrier has always been one of the keenest monitors, in constant contact with the one small device you keep on you at almost every moment. A confidential Facebook document reviewed by The Intercept shows that the social network courts carriers, along with phone makers — some 100 different companies in 50 countries — by offering the use of even more surveillance data, pulled straight from your smartphone by Facebook itself.

While AI is not explicitly mentioned in this article, the practice of data collection is closely tied to the implementation of AI techologies.

The article raises some important points on the ease of de-anonymizing anonymous data.

> From these eight categories alone, a third party could learn an extraordinary amount about patterns of users' daily life, and although the document claims that the data collected through the program is "aggregated and anonymized," academic studies have [found time and again](https://www.fastcompany.com/90278465/sorry-your-data-can-still-be-identified-even-its-anonymized) that so-called anonymized user data can be easily de-anonymized. Today, such claims of anonymization and aggregation are essentially boilerplate from companies who wager you'll be comfortable with them possessing a mammoth trove of personal observations and behavioral predictions about your past and future if the underlying data is sufficiently neutered and grouped with your neighbor's.

It also raises the important point of how such data can be used to systematically target or exclude certain demographics.

> Despite Facebook’s repeated assurances that user information is completely anonymized and aggregated, the Actionable Insights materials undermine this claim. One Actionable Insights case study from the overview document promotes how an unnamed North American cellular carrier had previously used its Actionable Insights access to target a specific, unnamed racial group. Facebook’s targeting of “multicultural affinity groups,” as the company formerly referred to race, was discontinued in 2017 after the targeting practice was widely criticized as potentially discriminatory.

> Another case study described how Actionable Insights can be used to single out individual customers on the basis of creditworthiness. In this example, Facebook explained how one of its advertising clients, based outside the U.S., wanted to exclude individuals from future promotional offers on the basis of their credit. Using data provided through Actionable Insights, a Data Science Strategist, a role for which Facebook continues to hire, was able to generate profiles of customers with desirable and undesirable credit standings. The advertising client then used these profiles to target or exclude Facebook users who resembled these profiles.

This then recalls the process of redlining, which was also raised by Joi Ito (see [here](https://www.wired.com/story/ideas-joi-ito-insurance-algorithms/)).

> This mechanism is also reminiscent of so-called redlining, the historical (and now illegal) practice of denying mortgages and other loans to marginalized groups on the basis of their demographics, according to Ashkan Sultani, a privacy researcher and former chief technologist of the Federal Trade Commission.

Finally, the article also raises the problem of lagging legislation that creates grey areas ripe for exploitation.

> According to Joel Reidenberg, a professor and director of Fordham’s Center on Law and Information Policy, Facebook’s credit-screening business seems to inhabit a fuzzy nether zone with regards to the FCRA, neither matching the legal definition of a credit agency nor falling outside the activities the law was meant to regulate. “It sure smells like the prescreening provisions of the FCRA,” Reidenberg told The Intercept. 

> Reidenberg explained that there are “all sorts of discrimination laws in terms of granting credit,” and that Facebook “may also be in a gray area with respect to those laws because they’re not offering credit, they’re offering an advertising space,” a distinction he described as “a very slippery slope.” An academic study [published in April](https://theintercept.com/2019/04/03/facebook-ad-algorithm-race-gender/) found that Facebook’s ad display algorithms were inherently biased with regards to gender and race.
