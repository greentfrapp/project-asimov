---
layout: post
title: "Algorithmic Bias? Big Data Refracting Society's Bias and Darkness"
ref: luo_2019
date: 2019-05-27 00:00:01
tags: algorithmic-bias chinese blackbox explainability
notes: True
---

# 算法歧视？大数据折射人类社会的偏见与阴暗

[Link to article](https://www.huxiu.com/article/272714.html)

This article begins with the news of Invictus Gaming's (IG) victory in a Dota2 tournament, with IG's boss Wang Sicong (王思聪) doing a lucky draw on Weibo (微博), the Chinese blogging site. Funny enough, out of the 113 lucky draw winners, 112 were female users with only 1 male user, while the participants' gender ratio was originally 1 male to 1.2 female. The Chinese participants eventually boiled it down to Weibo's algorithms treating many accounts as bot accounts and ruling them out from lucky draws.

> 这已经不是第一次人们质疑算法背后的“公正性”。在大数据时代，人类会有越来越多的利益分配和大数据直接相关；谁都不想在求职中被大数据自动判断为“缺乏竞争力的求职者”或者在法庭审判上被大数据抓进监狱。

> This is not the first time that people question the "fairness" behind algorithms. In the era of big data, the distribution of benefits will be increasingly associated with big data; nobody wants to be classified as "lacking competitiveness" or be sentenced to jail by big data.

> 然而事实却是，在算法决策的“黑匣子”面前，人类无法了解到算法的决策过程，而只能够了解到结果。

> But the truth is, faced with the "black box" of algorithms, people can no longer understand how a decision is made, they can only accept the decision.

The article raises the classic source of algorithm bias - biased data. In addition, the author also reasons that algorithmic bias is difficult to overcome. This, the author claims, is due to how algorithms work better on larger amounts of data. Combine this with the fact that the majority is likely to generate more data than the minority, the consequence is that algorithms will naturally perform better on majority demographics than minorities.

> 众所周知的是，当算法学习的数据量越大时，算法的错误会越少，而且结果会越趋向于精准。就算人类能够开发出一套筛选系统排除带偏见的数据，将不带偏见的数据输入算法中给算法学习，算法也无法达到绝对公平。

> 因为非主流总是拥有更少的数据，而主流永远拥有更多的数据；所以当两套算法相比较的时候，数据少的一方的错误会更多，而数据多的一方错误会更少，久而久之，两套算法之间的还是会拉开差距。

One interesting thing about the article is that despite its Chinese provenance, all of the examples cited are Western and US in particular, such as Google's classification of photos of black people as gorillas, Microsoft Tay turning anti-semitic and Amazon's sexist hiring algorithm. The only Chinese example is the one at the introduction of the article on Weibo's lucky draw. I wonder if this is because Chinese companies are less open/concerned about algorithmic bias or if the author is trying to show more diverse examples from his perspective.
